{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3b9y_9bmeguT"
   },
   "source": [
    "# Run Immediately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hjo4U9_g8ZTU"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /opt/conda/lib/python3.11/site-packages (3.1.1)\n",
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.11/site-packages (2.16.1)\n",
      "Requirement already satisfied: tensorrt in /opt/conda/lib/python3.11/site-packages (10.0.1)\n",
      "Collecting rdkit\n",
      "  Downloading rdkit-2023.9.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.11/site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from keras) (1.26.4)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (from keras) (13.7.1)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.11/site-packages (from keras) (0.0.7)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.11/site-packages (from keras) (3.10.0)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.11/site-packages (from keras) (0.11.0)\n",
      "Requirement already satisfied: ml-dtypes in /opt/conda/lib/python3.11/site-packages (from keras) (0.3.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (24.3.7)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from tensorflow) (24.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow) (69.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.10.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.62.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.36.0)\n",
      "Requirement already satisfied: tensorrt-cu12 in /opt/conda/lib/python3.11/site-packages (from tensorrt) (10.0.1)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.11/site-packages (from rdkit) (10.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
      "Downloading rdkit-2023.9.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.9/34.9 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: rdkit\n",
      "Successfully installed rdkit-2023.9.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install keras tensorflow tensorrt rdkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FHogdvuURMXm",
    "outputId": "c4456e2b-1e6e-4409-db67-d47d06ac2c3f"
   },
   "outputs": [],
   "source": [
    "# Normal Lib\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "# keras\n",
    "from keras import Model\n",
    "import keras\n",
    "from keras.layers import Conv3D, Input, MaxPooling3D, BatchNormalization, Dense, Dropout, Flatten, AveragePooling3D, Concatenate,Activation, GlobalAveragePooling3D\n",
    "\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.regularizers import L2\n",
    "\n",
    "# Metrics\n",
    "from scipy.stats import pearsonr, spearmanr # Pearson R best\n",
    "from keras.metrics import AUC, MeanAbsoluteError, Precision, Recall, Accuracy\n",
    "from sklearn.metrics import matthews_corrcoef, mean_squared_error, r2_score\n",
    "from keras.activations import linear\n",
    "# from sklearn.metrics import mean_squared_error,\n",
    "\n",
    "# rms = mean_squared_error(y_actual, y_predicted, squared=False)\n",
    "# Import File\n",
    "from zipfile import ZipFile\n",
    "import csv\n",
    "\n",
    "from rdkit import Chem\n",
    "\n",
    "# Import File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mounting drive onto colab\n",
    "# from zipfile import ZipFile\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive/')\n",
    "# dataset_folder = '/content/gdrive/MyDrive/Final'\n",
    "# files = os.listdir(dataset_folder)\n",
    "# for file in files:\n",
    "#   if '.zip' in file:\n",
    "#     file_path = os.path.join(dataset_folder, file)\n",
    "#     with ZipFile(file_path, 'r') as f:\n",
    "#       f.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/ligand-protein-tay-chan-mieng/Code\n"
     ]
    }
   ],
   "source": [
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0MVM63aTSsQ"
   },
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "fdtJ6OsX9xXK"
   },
   "outputs": [],
   "source": [
    "#Converts the protein-ligand complexes into 4D tensor.\n",
    "class Feature_extractor():\n",
    "    def __init__(self):\n",
    "        self.atom_codes = {}\n",
    "        #'others' includs metal atoms and B atom. There are no B atoms on training and test sets.\n",
    "\n",
    "        others = ([3,4,5,11,12,13]+list(range(19,32))+list(range(37,51))+list(range(55,84)))\n",
    "\n",
    "        # C and N atoms can be hybridized in three ways and S atom can be hybridized in two ways here.\n",
    "        # Hydrogen atom is also considered for feature extraction.\n",
    "\n",
    "        atom_types = [1,(6,1),(6,2),(6,3),(7,1),(7,2),(7,3),8,15,(16,2),(16,3),34,[9,17,35,53],others]\n",
    "\n",
    "        for i, j in enumerate(atom_types):\n",
    "            if type(j) is list:\n",
    "                for k in j:\n",
    "                    self.atom_codes[k] = i\n",
    "\n",
    "            else:\n",
    "                self.atom_codes[j] = i\n",
    "        self.sum_atom_types = len(atom_types)\n",
    "\n",
    "    #Onehot encoding of each atom. The atoms in protein or ligand are treated separately.\n",
    "    def encode(self, atomic_num, molprotein):\n",
    "        encoding = np.zeros(self.sum_atom_types*2)\n",
    "        if molprotein == 1:\n",
    "            encoding[self.atom_codes[atomic_num]] = 1.0\n",
    "        else:\n",
    "            encoding[self.sum_atom_types+self.atom_codes[atomic_num]] = 1.0\n",
    "\n",
    "        return encoding\n",
    "\n",
    "    #Get atom coords and atom features from the complexes.\n",
    "    def get_features(self, molecule, molprotein):\n",
    "        coords = []\n",
    "        features = []\n",
    "\n",
    "        # molecule = Chem.MolFromPDBFile(protein_test_path, False, False, 1)\n",
    "        molecule_conf = molecule.GetConformer()\n",
    "        molecule_positions = molecule_conf.GetPositions()\n",
    "\n",
    "        possible_hybridization_list = [\n",
    "        Chem.rdchem.HybridizationType.UNSPECIFIED,\n",
    "        Chem.rdchem.HybridizationType.S,\n",
    "        Chem.rdchem.HybridizationType.SP,\n",
    "        Chem.rdchem.HybridizationType.SP2,\n",
    "        Chem.rdchem.HybridizationType.SP3,\n",
    "        Chem.rdchem.HybridizationType.SP3D,\n",
    "        Chem.rdchem.HybridizationType.SP3D2\n",
    "        ]\n",
    "        for idx, pos in enumerate(molecule_positions):\n",
    "          coords.append(pos)\n",
    "          atom = molecule.GetAtomWithIdx(int(idx))\n",
    "          # print(\"A\")\n",
    "          # print(atom.GetHybridization())\n",
    "          if atom.GetAtomicNum() in [6,7,16]:\n",
    "\n",
    "            hyb = possible_hybridization_list.index(atom.GetHybridization())\n",
    "            if hyb < 1:\n",
    "              hyb = 2\n",
    "            atomicnum = (atom.GetAtomicNum(), hyb)\n",
    "            features.append(self.encode(atomicnum,molprotein))\n",
    "          else:\n",
    "            features.append(self.encode(atom.GetAtomicNum(),molprotein))\n",
    "\n",
    "        coords = np.array(coords, dtype=np.float32)\n",
    "        features = np.array(features, dtype=np.float32)\n",
    "\n",
    "        return coords, features\n",
    "\n",
    "    #Define the rotation matrixs of 3D stuctures.\n",
    "    def rotation_matrix(self, t, roller):\n",
    "        if roller==0:\n",
    "            return np.array([[1,0,0],[0,np.cos(t),np.sin(t)],[0,-np.sin(t),np.cos(t)]])\n",
    "        elif roller==1:\n",
    "            return np.array([[np.cos(t),0,-np.sin(t)],[0,1,0],[np.sin(t),0,np.cos(t)]])\n",
    "        elif roller==2:\n",
    "            return np.array([[np.cos(t),np.sin(t),0],[-np.sin(t),np.cos(t),0],[0,0,1]])\n",
    "\n",
    "    #Generate 3d grid or 4d tensor. Each grid represents a voxel. Each voxel represents the atom in it by onehot encoding of atomic type.\n",
    "    #Each complex in train set is rotated 9 times for data amplification.\n",
    "    #The complexes in core set are not rotated.\n",
    "    #The default resolution is 20*20*20.\n",
    "    def grid(self,coords, features, resolution=1.0, max_dist=10.0, rotations=9):\n",
    "        assert coords.shape[1] == 3\n",
    "        assert coords.shape[0] == features.shape[0]\n",
    "\n",
    "\n",
    "        grid=np.zeros((rotations+1,20,20,20,features.shape[1]),dtype=np.float32)\n",
    "        x=y=z=np.array(range(-10,10),dtype=np.float32)+0.5\n",
    "        for i in range(len(coords)):\n",
    "            coord=coords[i]\n",
    "            tmpx=abs(coord[0]-x)\n",
    "            tmpy=abs(coord[1]-y)\n",
    "            tmpz=abs(coord[2]-z)\n",
    "            if np.max(tmpx)<=19.5 and np.max(tmpy)<=19.5 and np.max(tmpz) <=19.5:\n",
    "                grid[0,np.argmin(tmpx),np.argmin(tmpy),np.argmin(tmpz)] += features[i]\n",
    "\n",
    "        for j in range(rotations):\n",
    "            theta = random.uniform(np.pi/18,np.pi/2)\n",
    "            roller = random.randrange(3)\n",
    "            coords = np.dot(coords, self.rotation_matrix(theta,roller))\n",
    "            for i in range(len(coords)):\n",
    "                coord=coords[i]\n",
    "                tmpx=abs(coord[0]-x)\n",
    "                tmpy=abs(coord[1]-y)\n",
    "                tmpz=abs(coord[2]-z)\n",
    "                if np.max(tmpx)<=19.5 and np.max(tmpy)<=19.5 and np.max(tmpz) <=19.5:\n",
    "                    grid[j+1,np.argmin(tmpx),np.argmin(tmpy),np.argmin(tmpz)] += features[i]\n",
    "\n",
    "        return grid\n",
    "\n",
    "    def update_grid(self, grid, x, coords, features, resolution=1.0, max_dist=10.0, rotations=9):\n",
    "        assert coords.shape[1] == 3\n",
    "        assert coords.shape[0] == features.shape[0]\n",
    "        y=z=x\n",
    "        for i in range(len(coords)):\n",
    "            coord=coords[i]\n",
    "            tmpx=abs(coord[0]-x)\n",
    "            tmpy=abs(coord[1]-y)\n",
    "            tmpz=abs(coord[2]-z)\n",
    "            if np.max(tmpx)<=19.5 and np.max(tmpy)<=19.5 and np.max(tmpz) <=19.5:\n",
    "                grid[0,np.argmin(tmpx),np.argmin(tmpy),np.argmin(tmpz)] += features[i]\n",
    "\n",
    "        for j in range(rotations):\n",
    "            theta = random.uniform(np.pi/18,np.pi/2)\n",
    "            roller = random.randrange(3)\n",
    "            coords = np.dot(coords, self.rotation_matrix(theta,roller))\n",
    "            for i in range(len(coords)):\n",
    "                coord=coords[i]\n",
    "                tmpx=abs(coord[0]-x)\n",
    "                tmpy=abs(coord[1]-y)\n",
    "                tmpz=abs(coord[2]-z)\n",
    "                if np.max(tmpx)<=19.5 and np.max(tmpy)<=19.5 and np.max(tmpz) <=19.5:\n",
    "                    grid[j+1,np.argmin(tmpx),np.argmin(tmpy),np.argmin(tmpz)] += features[i]\n",
    "\n",
    "        return grid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "yhV061ntqAgJ"
   },
   "outputs": [],
   "source": [
    "def get_atom_features(atom, amino_acid, isprotein):\n",
    "    ATOM_CODES = {}\n",
    "    metals = ([3, 4, 11, 12, 13] + list(range(19, 32))\n",
    "              + list(range(37, 51)) + list(range(55, 84))\n",
    "              + list(range(87, 104)))\n",
    "    atom_classes = [(5, 'B'), (6, 'C'), (7, 'N'), (8, 'O'), (15, 'P'), (16, 'S'), (34, 'Se'),\n",
    "                    ([9, 17, 35, 53], 'halogen'), (metals, 'metal')]\n",
    "    for code, (atomidx, name) in enumerate(atom_classes):\n",
    "        if type(atomidx) is list:\n",
    "            for a in atomidx:\n",
    "                ATOM_CODES[a] = code\n",
    "        else:\n",
    "            ATOM_CODES[atomidx] = code\n",
    "    try:\n",
    "        classes = ATOM_CODES[atom.GetAtomicNum()]\n",
    "    except:\n",
    "        classes = 9\n",
    "\n",
    "    possible_chirality_list = [\n",
    "        Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
    "        Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW,\n",
    "        Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW,\n",
    "        Chem.rdchem.ChiralType.CHI_OTHER\n",
    "    ]\n",
    "    chirality = possible_chirality_list.index(atom.GetChiralTag())\n",
    "\n",
    "    possible_formal_charge_list = [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]\n",
    "    try:\n",
    "        charge = possible_formal_charge_list.index(atom.GetFormalCharge())\n",
    "    except:\n",
    "        charge = 11\n",
    "\n",
    "    possible_hybridization_list = [\n",
    "        Chem.rdchem.HybridizationType.S,\n",
    "        Chem.rdchem.HybridizationType.SP,\n",
    "        Chem.rdchem.HybridizationType.SP2,\n",
    "        Chem.rdchem.HybridizationType.SP3,\n",
    "        Chem.rdchem.HybridizationType.SP3D,\n",
    "        Chem.rdchem.HybridizationType.SP3D2,\n",
    "        Chem.rdchem.HybridizationType.UNSPECIFIED\n",
    "    ]\n",
    "    try:\n",
    "        hyb = possible_hybridization_list.index(atom.GetHybridization())\n",
    "    except:\n",
    "        hyb = 6\n",
    "\n",
    "    possible_numH_list = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "    try:\n",
    "        numH = possible_numH_list.index(atom.GetTotalNumHs())\n",
    "    except:\n",
    "        numH = 9\n",
    "\n",
    "    possible_implicit_valence_list = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "    try:\n",
    "        valence = possible_implicit_valence_list.index(atom.GetTotalValence())\n",
    "    except:\n",
    "        valence = 8\n",
    "\n",
    "    possible_degree_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "    try:\n",
    "        degree = possible_degree_list.index(atom.GetTotalDegree())\n",
    "    except:\n",
    "        degree = 11\n",
    "\n",
    "    is_aromatic = [False, True]\n",
    "    aromatic = is_aromatic.index(atom.GetIsAromatic())\n",
    "\n",
    "    mass = atom.GetMass() / 100\n",
    "\n",
    "    amino_acids = [\n",
    "        'ALA', 'ARG', 'ASN', 'ASN', 'ASP', 'CYS', 'GLU', 'GLN', 'GLY', 'HIS', 'ILE', 'LEU', 'LYS', 'MET', 'PHE', 'PRO', 'SER', 'THR', 'TRP', 'TYR', 'VAL'\n",
    "    ]\n",
    "    if amino_acid in amino_acids:\n",
    "      amino_acid = amino_acids.index(amino_acid)\n",
    "    else:\n",
    "      amino_acid = int(len(amino_acids) + 1)\n",
    "\n",
    "    return [classes, chirality, charge, hyb, numH, valence, degree, aromatic, mass, amino_acid, isprotein]\n",
    "    # return [classes, chirality, charge, hyb, numH, valence, degree, aromatic, amino_acid, isprotein]\n",
    "\n",
    "def get_min_max(compound_positions):\n",
    "    minx,miny,minz = 999,999,999\n",
    "    maxx,maxy,maxz = -999,-999,-999\n",
    "    for pos in compound_positions:\n",
    "        x, y, z = pos\n",
    "        if x < minx:\n",
    "            minx = x\n",
    "\n",
    "        if y < miny:\n",
    "            miny = y\n",
    "\n",
    "        if z < minz:\n",
    "            minz = z\n",
    "\n",
    "        if x > maxx:\n",
    "            maxx = x\n",
    "\n",
    "        if y > maxy:\n",
    "            maxy = y\n",
    "\n",
    "        if z > maxz:\n",
    "            maxz = z\n",
    "\n",
    "    return (minx,miny,minz, maxx,maxy,maxz)\n",
    "\n",
    "def addNotUnique(nuC, uCL):\n",
    "  x,y,z = nuC\n",
    "\n",
    "  tmp1 = (x+1, y, z)\n",
    "  if tmp1 not in uCL:\n",
    "    uCL.append(tmp1)\n",
    "    return tmp1\n",
    "\n",
    "  tmp2 = (x, y+1, z)\n",
    "  if tmp2 not in uCL:\n",
    "    uCL.append(tmp2)\n",
    "    return tmp2\n",
    "\n",
    "  tmp3 = (x, y, z+1)\n",
    "  if tmp3 not in uCL:\n",
    "    uCL.append(tmp3)\n",
    "    return tmp3\n",
    "\n",
    "  tmp4 = (x+1, y+1, z)\n",
    "  if tmp4 not in uCL:\n",
    "    uCL.append(tmp4)\n",
    "    return tmp4\n",
    "\n",
    "  tmp5 = (x+1, y, z+1)\n",
    "  if tmp5 not in uCL:\n",
    "    uCL.append(tmp5)\n",
    "    return tmp5\n",
    "\n",
    "  tmp6 = (x, y+1, z+1)\n",
    "  if tmp6 not in uCL:\n",
    "    uCL.append(tmp6)\n",
    "    return tmp6\n",
    "\n",
    "  tmp7 = (x+1, y+1, z+1)\n",
    "  if tmp7 not in uCL:\n",
    "    uCL.append(tmp7)\n",
    "    return tmp7\n",
    "\n",
    "  tmp8 = (x-1, y, z)\n",
    "  if tmp8 not in uCL:\n",
    "    uCL.append(tmp8)\n",
    "    return tmp8\n",
    "\n",
    "  tmp9 = (x, y-1, z)\n",
    "  if tmp9 not in uCL:\n",
    "    uCL.append(tmp9)\n",
    "    return tmp9\n",
    "\n",
    "  tmp10 = (x, y, z-1)\n",
    "  if tmp10 not in uCL:\n",
    "    uCL.append(tmp10)\n",
    "    return tmp10\n",
    "\n",
    "  tmp11 = (x-1, y-1, z)\n",
    "  if tmp11 not in uCL:\n",
    "    uCL.append(tmp11)\n",
    "    return tmp11\n",
    "\n",
    "  tmp12 = (x-1, y, z-1)\n",
    "  if tmp12 not in uCL:\n",
    "    uCL.append(tmp12)\n",
    "    return tmp12\n",
    "\n",
    "  tmp13 = (x, y-1, z-1)\n",
    "  if tmp13 not in uCL:\n",
    "    uCL.append(tmp13)\n",
    "    return tmp13\n",
    "\n",
    "  tmp14 = (x-1, y-1, z-1)\n",
    "  if tmp14 not in uCL:\n",
    "    uCL.append(tmp14)\n",
    "    return tmp14\n",
    "\n",
    "def setup_grid(protein_path, ligand_path, isprotein=1):\n",
    "    compound = Chem.MolFromPDBFile(protein_path, False, False, 1)\n",
    "    compound_conf = compound.GetConformer()\n",
    "    compound_positions = compound_conf.GetPositions()\n",
    "\n",
    "    result = get_min_max(compound_positions)\n",
    "\n",
    "    atoms_aa = []\n",
    "    with open(protein_path, 'r+') as f:\n",
    "        readlines = f.readlines()\n",
    "        f.close()\n",
    "    for idx, lines in enumerate(readlines):\n",
    "        if 'HETATM' in lines or 'ATOM' in lines:\n",
    "            atoms_aa.append(lines[17:20])\n",
    "\n",
    "    with open(ligand_path, 'r+') as f:\n",
    "        readlines = f.readlines()\n",
    "        f.close()\n",
    "    for idx, lines in enumerate(readlines):\n",
    "        if 'HETATM' in lines or 'ATOM' in lines:\n",
    "            atoms_aa.append(lines[17:20])\n",
    "    # print(result)\n",
    "    # centerx = result[3] - result[0]\n",
    "    # centery = result[4] - result[1]\n",
    "    # centerz = result[5] - result[2]\n",
    "    # print((centerx,centery,centerz))\n",
    "\n",
    "    uniqueCoord = []\n",
    "    # repeatedCoord = []\n",
    "\n",
    "    atom_e = compound.GetAtomWithIdx(int(1))\n",
    "    features_e = get_atom_features(atom_e, '', 1)\n",
    "    grid=np.zeros((52,52,52,len(features_e)+3))\n",
    "    # print(np.shape(grid))\n",
    "\n",
    "    for idx, coords in enumerate(compound_positions):\n",
    "      amino_acid = atoms_aa[idx]\n",
    "      atom = compound.GetAtomWithIdx(int(idx))\n",
    "      features = get_atom_features(atom, amino_acid, 1)\n",
    "      # features.append(coords[0])\n",
    "      # features.append(coords[1])\n",
    "      # features.append(coords[2])\n",
    "      features.extend([coords[0],coords[1],coords[2]])\n",
    "      # print(idx)\n",
    "      # print(coords)\n",
    "      # print(\"-----------\")\n",
    "      x= coords[0] - (result[0])\n",
    "      y= coords[1] - (result[1])\n",
    "      z= coords[2] - (result[2])\n",
    "      roundx = round(x)\n",
    "      roundy = round(y)\n",
    "      roundz = round(z)\n",
    "      checkCoords = (roundx, roundy, roundz)\n",
    "      # checkCoords2 = (x,y,z)\n",
    "      if checkCoords not in uniqueCoord:\n",
    "        uniqueCoord.append(checkCoords)\n",
    "        grid[roundx, roundy, roundz] = features\n",
    "      else:\n",
    "        # repeatedCoord.append(checkCoords2)\n",
    "        tmpCoord = addNotUnique(checkCoords, uniqueCoord)\n",
    "        grid[tmpCoord[0], tmpCoord[1], tmpCoord[2]] = features\n",
    "\n",
    "    con_com = len(compound_positions)\n",
    "    compound = Chem.MolFromPDBFile(ligand_path, False, False, 1)\n",
    "    compound_conf = compound.GetConformer()\n",
    "    compound_positions = compound_conf.GetPositions()\n",
    "\n",
    "    for idx, coords in enumerate(compound_positions):\n",
    "      amino_acid = atoms_aa[idx+con_com]\n",
    "      atom = compound.GetAtomWithIdx(int(idx))\n",
    "      features = get_atom_features(atom, amino_acid, 0)\n",
    "      features.extend([coords[0],coords[1],coords[2]])\n",
    "\n",
    "      x= coords[0] - (result[0])\n",
    "      y= coords[1] - (result[1])\n",
    "      z= coords[2] - (result[2])\n",
    "      roundx = round(x)\n",
    "      roundy = round(y)\n",
    "      roundz = round(z)\n",
    "      checkCoords = (roundx, roundy, roundz)\n",
    "      # checkCoords2 = (x,y,z)\n",
    "      if checkCoords not in uniqueCoord:\n",
    "        uniqueCoord.append(checkCoords)\n",
    "        grid[roundx, roundy, roundz] = features\n",
    "      else:\n",
    "        # repeatedCoord.append(checkCoords2)\n",
    "        tmpCoord = addNotUnique(checkCoords, uniqueCoord)\n",
    "        grid[tmpCoord[0], tmpCoord[1], tmpCoord[2]] = features\n",
    "\n",
    "\n",
    "    return grid\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yq6rr80ME2JU"
   },
   "source": [
    "## Get data batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "4NN8v1ECAmco"
   },
   "outputs": [],
   "source": [
    "def get_data_batch(dataset_idx, protein_folder, ligand_folder, label_folder, batch_size, index, type_model):\n",
    "  core_grids=None\n",
    "  core_ba= []\n",
    "  core_stat= []\n",
    "  # List all files and directories in the dataset folder\n",
    "  ligand_list = os.listdir(ligand_folder)\n",
    "  protein_list = os.listdir(protein_folder)\n",
    "  label_list = os.listdir(label_folder)\n",
    "\n",
    "  # Get the batch list by index\n",
    "  batch_list = [value for idx, value in enumerate(dataset_idx) if idx >= index * batch_size and idx < (index+1)*batch_size ]\n",
    "  if type_model == '3DCNN':\n",
    "    for i in batch_list:\n",
    "      # Read all the files in ligand_folder as a list\n",
    "      # Feature = gridFromCenter()\n",
    "      complexFile = ligand_list[i]\n",
    "\n",
    "      # Get Data and Labels\n",
    "      ## Get the protein name from the complex file, ex: 3qzq-8v_model1.pdb -> 3qzq\n",
    "      from_protein = complexFile.split('-')[0]\n",
    "      # Get the complex name from the complex file, ex: 3qzq-8v_model1.pdb -> 3qzq-8v_model1\n",
    "      complex_name = complexFile.split('.')[0]\n",
    "      # Get the path to all .pdb files in protein folder\n",
    "      protein_path = os.path.join(protein_folder, from_protein+'.pdb')\n",
    "      complexFile_path = os.path.join(ligand_folder, complexFile)\n",
    "\n",
    "      # grid, minx, miny, minz = set_grid(protein_path)\n",
    "      # grid = add_ligand(complexFile_path, grid, minx, miny, minz)\n",
    "      grid = setup_grid(protein_path, complexFile_path)\n",
    "      if core_grids is None:\n",
    "          core_grids = []\n",
    "      core_grids.append(grid)\n",
    "      grid = []\n",
    "\n",
    "      protein = [value for value in label_list if from_protein in value][0]\n",
    "      label_file_path = os.path.join(label_folder, protein)\n",
    "\n",
    "      df = pd.read_csv(label_file_path)\n",
    "      listidx = df.index[df['file.pdb'] == complex_name].tolist()[0]\n",
    "      ba = df['BA'][listidx]\n",
    "      stat = df['Hit/No_hit'][listidx]\n",
    "      if stat == 'hit':\n",
    "        stat = 1\n",
    "      else:\n",
    "        stat = 0\n",
    "      core_ba.append(ba)\n",
    "      core_stat.append(stat)\n",
    "\n",
    "    core_grids = np.array(core_grids)\n",
    "    core_ba = np.array(core_ba)\n",
    "    core_stat = np.array(core_stat)\n",
    "\n",
    "    return core_grids, core_ba, core_stat\n",
    "\n",
    "\n",
    "  if type_model == 'SFCNN':\n",
    "    Feature = Feature_extractor()\n",
    "    for id in batch_list:\n",
    "      # if cur_batch*batch_size >=\n",
    "      ligand_name = ligand_list[id]\n",
    "      ligand_train_path = os.path.join(ligand_folder, ligand_name)\n",
    "      # print(ligand_name)\n",
    "      protein_name1 = ligand_name.split('-')[0]\n",
    "      protein_name2 = protein_name1\n",
    "      complex_name = ligand_name.split('.')[0]\n",
    "      for name in protein_list:\n",
    "        if protein_name1 in name:\n",
    "          protein_name1 = name\n",
    "          continue\n",
    "      # print(protein_folder)\n",
    "      protein_train_path = os.path.join(protein_folder, protein_name1)\n",
    "\n",
    "      protein = Chem.MolFromPDBFile(protein_train_path, False, False, 0)\n",
    "      ligand = Chem.MolFromPDBFile(ligand_train_path, False, False, 0)\n",
    "      # train_complexes.append((protein, ligand))\n",
    "      coords1, features1 = Feature.get_features(protein,1)\n",
    "      coords2, features2 = Feature.get_features(ligand,0)\n",
    "      protein = None\n",
    "      ligand = None\n",
    "      center=(np.max(coords2,axis=0)+np.min(coords2,axis=0))/2\n",
    "      coords=np.concatenate([coords1,coords2],axis = 0)\n",
    "      features=np.concatenate([features1,features2],axis = 0)\n",
    "      assert len(coords) == len(features)\n",
    "      coords = coords-center\n",
    "      grid=Feature.grid(coords,features, rotations=0)\n",
    "      if core_grids is None:\n",
    "          core_grids = grid\n",
    "      else:\n",
    "          core_grids = np.concatenate([core_grids,grid],axis = 0)\n",
    "      grid = []\n",
    "\n",
    "      label_list = os.listdir(label_folder)\n",
    "      for name in label_list:\n",
    "        if protein_name2 in name:\n",
    "          protein_name2 = name\n",
    "          continue\n",
    "      label_train_path = os.path.join(label_folder, protein_name2)\n",
    "      df = pd.read_csv(label_train_path)\n",
    "      listidx = df.index[df['file.pdb'] == complex_name].tolist()[0]\n",
    "      ba = df['BA'][listidx]\n",
    "      stat = df['Hit/No_hit'][listidx]\n",
    "      if stat == 'hit':\n",
    "        stat = 1\n",
    "      else:\n",
    "        stat = 0\n",
    "\n",
    "      core_ba.append(ba)\n",
    "      core_stat.append(stat)\n",
    "\n",
    "    core_ba = np.array(core_ba)\n",
    "    core_stat = np.array(core_stat)\n",
    "    return core_grids, core_ba, core_stat\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jP70igtsHsUf"
   },
   "source": [
    "## Other functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "MHvV9Ca6Ap5c"
   },
   "outputs": [],
   "source": [
    "def model_train(model, train_dataset_idx, protein_folder, ligand_folder, label_folder, batch_size, epochs, save_path, best_path, type_model):\n",
    "  dataset_len = len(train_dataset_idx)\n",
    "  runs = dataset_len // batch_size\n",
    "  cur = 1\n",
    "  log_txt = \"log.txt\"\n",
    "  log_path = os.path.join(save_path, log_txt)\n",
    "  readline = ''\n",
    "  if os.path.exists(log_path):\n",
    "    log_file = open(log_path,\"r+\")\n",
    "    readline = log_file.readline()\n",
    "    log_file.close()\n",
    "  else:\n",
    "    with open(log_path, 'w+') as f:\n",
    "      f.write('0/'+str(runs))\n",
    "      f.close()\n",
    "\n",
    "  if readline == '' or int(readline.split('/')[0]) > runs or int(readline.split('/')[0]) == 0:\n",
    "    cur = 1\n",
    "    model.save(save_path + '.keras')\n",
    "  else:\n",
    "    cur = int(readline.split('/')[0])\n",
    "  check = 0\n",
    "\n",
    "  print(\"----------------------- Start TrainDataset ------------------------\")\n",
    "  for i in range(int(cur-1),int(runs)):\n",
    "    print(\"=======================Batch \"+ str(i+1)+\"==============================\")\n",
    "    model = keras.models.load_model(save_path + '.keras')\n",
    "    print(\"Get dataset\")\n",
    "    gridList, baList, statList = get_data_batch(train_dataset_idx, protein_folder, ligand_folder, label_folder, batch_size, i, type_model)\n",
    "    print(\"----------------------- Train TrainDataset ------------------------\")\n",
    "    model.fit(gridList, [baList, statList], epochs= epochs,verbose=0)\n",
    "    gridList, baList, statList = [], [], []\n",
    "    # PearsonR, MSE, RMSE, precision, recall, auc, f1_score, MCC = model_val_dataset(val_dataset_idx, protein_folder, label_folder, label_list, batch_size, epochs, save_path, best_path)\n",
    "    print(\"Save\")\n",
    "    model.save(save_path + '.keras')\n",
    "    log_file = open(log_path,\"r+\")\n",
    "    readline = log_file.write(str(i)+'/'+str(runs))\n",
    "    log_file.close()\n",
    "    # if PearsonR > checkPearsonR and MCC > checkMCC and RMSE < checkRMSE:\n",
    "    #   model.save(best_path)\n",
    "    #   checkPearsonR = PearsonR\n",
    "    #   checkMCC = MCC\n",
    "    #   checkRMSE = RMSE\n",
    "    if check == 0 and batch_size*i >= 2000:\n",
    "      check +=1\n",
    "      model.save(best_path + '.keras')\n",
    "    print(\"======================= End Batch \"+ str(i+1)+\"==============================\")\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_xPW_OVxA1ET"
   },
   "outputs": [],
   "source": [
    "def SFCNN_model(input_shape=(20,20,20,28)):\n",
    "  inp = Input(shape= input_shape, name='Input_Complexes')\n",
    "\n",
    "  # Classification\n",
    "\n",
    "  ## Check there are atoms\n",
    "  x1 = Conv3D(7,kernel_size=(1,1,1),strides=(1,1,1))(inp)\n",
    "  x1 = BatchNormalization()(x1)\n",
    "  x1 = Activation('relu')(x1)\n",
    "\n",
    "  x1 = Conv3D(7,kernel_size=(3,3,3))(x1)\n",
    "  x1 = BatchNormalization()(x1)\n",
    "  x1 = Activation('relu')(x1)\n",
    "  # x1 = AveragePooling3D(padding='same')(x1)\n",
    "\n",
    "  x1 = Conv3D(56,kernel_size=(3,3,3),padding='same')(x1)\n",
    "  x1 = BatchNormalization()(x1)\n",
    "  x1 = Activation('relu')(x1)\n",
    "  x1 = MaxPooling3D(pool_size=2)(x1)\n",
    "\n",
    "  x1 = Conv3D(112,kernel_size=(3,3,3),padding='same')(x1)\n",
    "  x1 = BatchNormalization()(x1)\n",
    "  x1 = Activation('relu')(x1)\n",
    "  x1 = MaxPooling3D(pool_size=2)(x1)\n",
    "\n",
    "  x1 = Conv3D(224,kernel_size=(3,3,3),padding='same')(x1)\n",
    "  x1 = BatchNormalization()(x1)\n",
    "  x1 = Activation('relu')(x1)\n",
    "  x1 = MaxPooling3D(pool_size=2)(x1)\n",
    "\n",
    "  # Global Pooling\n",
    "  x2 = GlobalAveragePooling3D()(x1)\n",
    "\n",
    "  x2 = Dense(256)(x2)\n",
    "  x2 = BatchNormalization()(x2)\n",
    "  x2 = Activation('relu')(x2)\n",
    "  x2 = Dropout(0.5)(x2)\n",
    "\n",
    "  # Flattening\n",
    "  x1 = Flatten()(x1)\n",
    "\n",
    "  x1 = Dense(256)(x1)\n",
    "  x1 = BatchNormalization()(x1)\n",
    "  x1 = Activation('relu')(x1)\n",
    "  x1 = Dropout(0.5)(x1)\n",
    "\n",
    "  # Regression Output\n",
    "  d1 = Dense(1,kernel_regularizer=tf.keras.regularizers.l2(0.01))(x1)\n",
    "  # Classification Output\n",
    "  d2 = Dense(1, activation='sigmoid')(x2)\n",
    "\n",
    "  return Model(inputs=[inp], outputs=[d1,d2], name='Embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "5qVuOG5duw8Q"
   },
   "outputs": [],
   "source": [
    "def CNN_model(drop_rate, input_shape= (52,52,52,14)):\n",
    "  inp = Input(shape= input_shape, name='Input_Complexes')\n",
    "\n",
    "  ## Check there are atoms\n",
    "  ## Sketch the pattern of the whole biomolecule\n",
    "  x1 = Conv3D(filters= 32, kernel_size=(1,1,1),strides=(1,1,1) ,padding='same', bias_initializer='zeros', kernel_initializer='glorot_uniform')(inp)\n",
    "  x1 = BatchNormalization()(x1)\n",
    "  x1 = Activation('relu')(x1)\n",
    "\n",
    "  x1 = Conv3D(filters= 8, kernel_size=2, padding='same')(x1)\n",
    "  x1 = BatchNormalization()(x1)\n",
    "  x1 = Activation('relu')(x1)\n",
    "\n",
    "  x1 = Conv3D(filters= 8, kernel_size=3,padding='same')(x1)\n",
    "  x1 = BatchNormalization()(x1)\n",
    "  x1 = Activation('relu')(x1)\n",
    "\n",
    "  x1 = MaxPooling3D(pool_size=2)(x1)\n",
    "\n",
    "  ## Find pattern for chunk size\n",
    "\n",
    "  x1 = Conv3D(filters= 128,kernel_size=(1,1,1),padding='same')(x1)\n",
    "  x1 = BatchNormalization()(x1)\n",
    "  x1 = Activation('relu')(x1)\n",
    "\n",
    "  x1 = Conv3D(filters= 64,kernel_size=2,padding='same')(x1)\n",
    "  x1 = BatchNormalization()(x1)\n",
    "  x1 = Activation('relu')(x1)\n",
    "\n",
    "  x1 = Conv3D(filters= 64,kernel_size=3,padding='same')(x1)\n",
    "  x1 = BatchNormalization()(x1)\n",
    "  x1 = Activation('relu')(x1)\n",
    "\n",
    "  x1 = MaxPooling3D(pool_size=2)(x1)\n",
    "\n",
    "  ## Find pattern for amino acid size\n",
    "  x1 = Conv3D(filters= 256,kernel_size=(1,1,1),padding='same')(x1)\n",
    "  x1 = BatchNormalization()(x1)\n",
    "  x1 = Activation('relu')(x1)\n",
    "\n",
    "  x1 = Conv3D(filters= 128,kernel_size=2,padding='same')(x1)\n",
    "  x1 = BatchNormalization()(x1)\n",
    "  x1 = Activation('relu')(x1)\n",
    "\n",
    "  x1 = Conv3D(filters= 128,kernel_size=3,padding='same')(x1)\n",
    "  x1 = BatchNormalization()(x1)\n",
    "  x1 = Activation('relu')(x1)\n",
    "\n",
    "  x1 = MaxPooling3D(pool_size=2)(x1)\n",
    "\n",
    "  ## Find pattern for atom interaction size\n",
    "  x1 = Conv3D(filters= 512,kernel_size=(1,1,1),padding='same')(x1)\n",
    "  x1 = BatchNormalization()(x1)\n",
    "  x1 = Activation('relu')(x1)\n",
    "\n",
    "  x1 = Conv3D(filters= 256,kernel_size=2,padding='same')(x1)\n",
    "  x1 = BatchNormalization()(x1)\n",
    "  x1 = Activation('relu')(x1)\n",
    "\n",
    "  x1 = Conv3D(filters= 256,kernel_size=3,padding='same')(x1)\n",
    "  x1 = BatchNormalization()(x1)\n",
    "  x1 = Activation('relu')(x1)\n",
    "\n",
    "  # x1 = MaxPooling3D(pool_size=2)(x1)\n",
    "\n",
    "  #   # Global Pooling\n",
    "  x2 = GlobalAveragePooling3D()(x1)\n",
    "\n",
    "  x2 = Dense(256)(x2)\n",
    "  x2 = BatchNormalization()(x2)\n",
    "  x2 = Activation('relu')(x2)\n",
    "  x2 = Dropout(0.5)(x2)\n",
    "\n",
    "  # # Flattening\n",
    "  x1 = Flatten()(x1)\n",
    "\n",
    "  x1 = Dense(256)(x1)\n",
    "  x1 = BatchNormalization()(x1)\n",
    "  x1 = Activation('relu')(x1)\n",
    "  x1 = Dropout(0.5)(x1)\n",
    "\n",
    "  # # Regression Output\n",
    "  d1 = Dense(1,kernel_regularizer=tf.keras.regularizers.l2(0.01))(x1)\n",
    "  # Classification Output\n",
    "  d2 = Dense(1, activation='sigmoid')(x2)\n",
    "\n",
    "  # return Model(inputs=[inp], outputs=[d1,d2], name='Embedding')\n",
    "  return Model(inputs=[inp], outputs=[d1, d2], name='Embedding')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "SrJN6m2bMRrt"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Eamyt3xQLo70"
   },
   "outputs": [],
   "source": [
    "def plot_class(x, y):\n",
    "  #create scatterplot with regression line\n",
    "  # plt.scatter(x, y)\n",
    "  # plt.show()\n",
    "  # # sns.regplot(y_label, y_pred)\n",
    "  new_y = []\n",
    "  for value in y:\n",
    "    if value >= 0.9:\n",
    "      new_y.append(1)\n",
    "    else:\n",
    "      new_y.append(0)\n",
    "  confusion_matrix = metrics.confusion_matrix(x, new_y)\n",
    "\n",
    "  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [\"Hit\", \"No_Hit\"])\n",
    "\n",
    "  cm_display.plot()\n",
    "  plt.show()\n",
    "  return cm_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "eZlT-8SFRC12"
   },
   "outputs": [],
   "source": [
    "def plot_reg(x, y):\n",
    "  #create scatterplot with regression line\n",
    "  #use green as color for individual points\n",
    "  X = np.array(x)\n",
    "  plt.plot(x, y, 'o', color='green')\n",
    "\n",
    "  #obtain m (slope) and b(intercept) of linear regression line\n",
    "  m, b = np.polyfit(x, y, 1)\n",
    "\n",
    "\n",
    "  #use red as color for regression line\n",
    "  plt.plot(X, m*X+b, color='red')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "GfeECylChzQj"
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "7Ck4nOLFBDgl"
   },
   "outputs": [],
   "source": [
    "def get_metrics(y_label, y_pred, ytype):\n",
    "\n",
    "  if ytype == 0: # Regression\n",
    "    print(\"++++++++++++++++++++++++++Regression++++++++++++++++++++++++++\")\n",
    "    PearsonR, p1 = pearsonr(y_label, y_pred)\n",
    "    print('Pearson Correlation Coefficient: ' + str(PearsonR))\n",
    "    print('P value: ' + str(p1))\n",
    "    MSE = MeanAbsoluteError()\n",
    "    MSE.update_state(y_label, y_pred)\n",
    "    MSE = MSE.result().numpy()\n",
    "    print('Mean Absolute Error: ' + str(MSE))\n",
    "\n",
    "    rms = mean_squared_error(y_label, y_pred, squared=False)\n",
    "    # rms = math.sqrt(mean_squared_error(y_label, y_pred, squared=False))\n",
    "    print('Root Mean Error: ' + str(rms))\n",
    "\n",
    "    r2 =r2_score(y_label, y_pred)\n",
    "    print('Correlation of Covariance: ' + str(r2))\n",
    "\n",
    "    rho, p2 = spearmanr(y_label, y_pred)\n",
    "    print('Spearman Rank Correlation Coefficient: ' + str(rho))\n",
    "    print('P value: ' + str(p2))\n",
    "\n",
    "    # plot_reg(y_label, y_pred)\n",
    "\n",
    "    return PearsonR, p1, MSE, rms, rho, p2\n",
    "\n",
    "  if ytype == 1: # Classification\n",
    "    print(\"+++++++++++++++++++++++Classification+++++++++++++++++++++++\")\n",
    "\n",
    "    tp = keras.metrics.TruePositives(thresholds= 0.9)\n",
    "    tn = keras.metrics.TrueNegatives(thresholds= 0.9)\n",
    "    fp = keras.metrics.FalsePositives(thresholds= 0.9)\n",
    "    fn = keras.metrics.FalseNegatives(thresholds= 0.9)\n",
    "\n",
    "\n",
    "    tp.update_state(y_label, y_pred)\n",
    "    tn.update_state(y_label, y_pred)\n",
    "    fp.update_state(y_label, y_pred)\n",
    "    fn.update_state(y_label, y_pred)\n",
    "\n",
    "    tp = tp.result().numpy()\n",
    "    tn = tn.result().numpy()\n",
    "    fp = fp.result().numpy()\n",
    "    fn = fn.result().numpy()\n",
    "\n",
    "    precision = tp/ (tp+fp) # PPV\n",
    "    print('Precision: ' + str(precision))\n",
    "\n",
    "    recall = tp/(tp+fn) # Recall - TPR\n",
    "    print('Recall: ' + str(recall))\n",
    "\n",
    "    specificity = tn/(tn+fp)\n",
    "    print('Specificity: ' + str(specificity))\n",
    "\n",
    "    NPV = tn/(tn+fn)\n",
    "    print('NPV: ' + str(NPV))\n",
    "\n",
    "    MCC = (tp*tn - fp*fn)/ math.sqrt(   (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)  ) # Phi coefficient\n",
    "    print(\"Phi coefficient:\" + str(MCC))\n",
    "\n",
    "    return precision, recall, specificity, NPV, MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "6LKmRzhNBHQ3"
   },
   "outputs": [],
   "source": [
    "def model_val_dataset(val_dataset_idx, protein_folder, ligand_folder, label_folder, batch_size, epochs, save_path, best_path, type_model):\n",
    "  # dataset_len = len(val_dataset_idx)\n",
    "  # runs = dataset_len // batch_size\n",
    "  # model = keras.models.load_model(save_path)\n",
    "  dataset_len = len(val_dataset_idx)\n",
    "  runs = dataset_len // batch_size\n",
    "  last_batch = dataset_len - batch_size*runs\n",
    "  model = keras.models.load_model(save_path + '.keras')\n",
    "  # PearsonR_list, MCC_list, RSME_list = 0,0,0\n",
    "  ba_Actual, stat_Actual, ba_Pred, stat_Pred = [], [], [], []\n",
    "\n",
    "  print(\"----------------------- Start ValDataset ------------------------\")\n",
    "  for i in range(int(runs+1)):\n",
    "\n",
    "    print(\"Get dataset on batch \"+str(i+1))\n",
    "    if i != runs+1:\n",
    "      gridList, baList, statList = get_data_batch(val_dataset_idx, protein_folder, ligand_folder, label_folder, batch_size, i, type_model)\n",
    "    else:\n",
    "      gridList, baList, statList = get_data_batch(val_dataset_idx, protein_folder, ligand_folder, label_folder, last_batch, i, type_model)\n",
    "\n",
    "    print(\"----------------------- Predict ValDataset ------------------------\")\n",
    "    result = model.predict(gridList, verbose=2)\n",
    "    gridList = []\n",
    "    pred_reg, pred_class = result\n",
    "\n",
    "    baList = [value for value in baList.tolist()]\n",
    "    statList = [value for value in statList.tolist()]\n",
    "\n",
    "    pred_reg = [value[0] for value in pred_reg.tolist()]\n",
    "    pred_class = [value[0] for value in pred_class.tolist()]\n",
    "\n",
    "    if ba_Actual == []:\n",
    "      ba_Actual = baList\n",
    "      stat_Actual = statList\n",
    "      ba_Pred = pred_reg\n",
    "      stat_Pred = pred_class\n",
    "    else:\n",
    "      ba_Actual.extend(baList)\n",
    "      stat_Actual.extend(statList)\n",
    "      ba_Pred.extend(pred_reg)\n",
    "      stat_Pred.extend(pred_class)\n",
    "\n",
    "    baList, statList = [], []\n",
    "\n",
    "  reg_res = get_metrics(ba_Actual, ba_Pred, 0)\n",
    "  print(\"-------------------------------------------------------------\")\n",
    "  class_res = get_metrics(stat_Actual, stat_Pred, 1)\n",
    "\n",
    "  return ba_Actual, ba_Pred, stat_Actual, stat_Pred, reg_res, class_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "NJR3ABEHELvX"
   },
   "outputs": [],
   "source": [
    "def exportCSV(ligand_folder,pred_list,result,csv_path):\n",
    "  fields = ['Model', 'Hit_Label', 'Hit_Prediction', 'BindingAffinity_Label', 'BindingAffinity_Prediction']\n",
    "  rows = []\n",
    "  ligand_list = os.listdir(ligand_folder)\n",
    "  for idx, value in enumerate(pred_list):\n",
    "    row = []\n",
    "    row.append(ligand_list[value])\n",
    "    row.append(result[0][idx])\n",
    "    row.append(result[1][idx])\n",
    "    row.append(result[2][idx])\n",
    "    row.append(result[3][idx])\n",
    "\n",
    "    rows.append(row)\n",
    "\n",
    "  with open(csv_path, 'w') as csvfile:\n",
    "    # creating a csv writer object\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "\n",
    "    # writing the fields\n",
    "    csvwriter.writerow(fields)\n",
    "\n",
    "    # writing the data rows\n",
    "    csvwriter.writerows(rows)\n",
    "  csvfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wwckd8Tluiub"
   },
   "source": [
    "## Confirmed Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "bJTrS2ruipNM"
   },
   "outputs": [],
   "source": [
    "def hyper_train(train_dataset_idx, protein_folder, ligand_folder, label_folder, batch_size, epochs, save_path, best_path, type_model, hyper_choices):\n",
    "  model = None\n",
    "  if type_model == 'SFCNN':\n",
    "    model = SFCNN_model()\n",
    "  if type_model == '3DCNN':\n",
    "    model = CNN_model(0.5)\n",
    "\n",
    "\n",
    "  if model != None:\n",
    "    batch_size, epochs, optimizer, loss = hyper_choices\n",
    "    model.compile(optimizer= optimizer,\n",
    "                  loss= loss)\n",
    "    model_train(model, train_dataset_idx, protein_folder, ligand_folder, label_folder, batch_size, epochs, save_path, best_path, type_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "fxMlJx8ff_u3"
   },
   "outputs": [],
   "source": [
    "def exportVal(val_dataset_idx, protein_folder, ligand_folder, label_folder, batch_size, epochs, save_path, best_path, type_model, csv_path):\n",
    "  result = model_val_dataset(val_dataset_idx, protein_folder, ligand_folder, label_folder, batch_size, epochs, save_path, best_path, type_model)\n",
    "  exportCSV(ligand_folder,val_dataset_idx,result,csv_path)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "BNZ53e5LFj94"
   },
   "outputs": [],
   "source": [
    "def test_model(train_dataset_idx, protein_folder, ligand_folder, label_folder, batch_size, epochs, save_path, best_path, type_model, hyper_choices):\n",
    "  model = None\n",
    "  if type_model == 'SFCNN':\n",
    "    model = SFCNN_model()\n",
    "  if type_model == '3DCNN':\n",
    "    model = CNN_model(0.5)\n",
    "\n",
    "\n",
    "  if model != None:\n",
    "    batch_size, epochs, optimizer, loss = hyper_choices\n",
    "    model.compile(optimizer= optimizer,\n",
    "                  loss= loss)\n",
    "    model.summary()\n",
    "#     model.save('/content/temp')\n",
    "    for i in range(312):\n",
    "      print(\"Batch \"+ str(i+1))\n",
    "#       model = keras.models.load_model('/content/temp')\n",
    "      gridList, baList, statList = get_data_batch(train_dataset_idx, protein_folder, ligand_folder, label_folder, batch_size, i, type_model)\n",
    "      model.fit(gridList, [baList, statList], epochs= epochs,verbose=0)\n",
    "      gridList, baList, statList = [], [], []\n",
    "#       model.save('/content/temp')\n",
    "#     # model_train(model, train_dataset_idx, protein_folder, ligand_folder, label_folder, batch_size, epochs, save_path, best_path, type_model)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BTqd63HuAWY9"
   },
   "source": [
    "## Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "7FqyqgLaAWMV"
   },
   "outputs": [],
   "source": [
    "p_folder = './protein'\n",
    "p_list = os.listdir(p_folder)\n",
    "\n",
    "l_folder = './ligand'\n",
    "l_list = os.listdir(l_folder)\n",
    "\n",
    "la_folder = './label'\n",
    "la_list = os.listdir(la_folder)\n",
    "\n",
    "protein_test_path = './protein/3qzq.pdb'\n",
    "\n",
    "totalSize = len(l_list)\n",
    "totalSize\n",
    "\n",
    "permu = np.random.RandomState(seed=69).permutation(totalSize)\n",
    "\n",
    "train_num, validate_num, test_num = 0,0,0\n",
    "iDataset_num = totalSize\n",
    "ratio = (60,20,20)\n",
    "\n",
    "train_num = int(iDataset_num * (ratio[0]/ (ratio[0]+ratio[1]+ratio[2])))\n",
    "val_num = int(iDataset_num * (ratio[1]/ (ratio[0]+ratio[1]+ratio[2])))\n",
    "test_num = int(iDataset_num * (ratio[2]/ (ratio[0]+ratio[1]+ratio[2])))\n",
    "\n",
    "val_num = 100\n",
    "test_num = 500\n",
    "last_num = 2000\n",
    "\n",
    "train_list_IDs = permu[:train_num]\n",
    "val_list_IDs = permu[train_num:(train_num+val_num)]\n",
    "test_list_IDs = permu[(train_num+val_num):(train_num+val_num+test_num)]\n",
    "last_list_IDs = permu[(train_num+val_num+test_num):(train_num+val_num+test_num+last_num)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8gAacw0NUsKx",
    "outputId": "86b1ddba-c3bd-499d-a1a5-54e759b1d335"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1592, 14669,  5474, ..., 14740,  9818,  4041])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XWBD0NVvU7DP",
    "outputId": "76d932b2-24f0-4c48-9b2e-4c0674dd8e3c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1592, 14669,  5474, ...,  3035,  6871,  6499])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_list_IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejfuHz0p7JOS"
   },
   "source": [
    "## Setup Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "nsDcAvTN7I8p"
   },
   "outputs": [],
   "source": [
    "def cus_class_loss(y_label, y_pred):\n",
    "    tp = keras.metrics.TruePositives(thresholds= 0.9)\n",
    "    tn = keras.metrics.TrueNegatives(thresholds= 0.9)\n",
    "    fp = keras.metrics.FalsePositives(thresholds= 0.9)\n",
    "    fn = keras.metrics.FalseNegatives(thresholds= 0.9)\n",
    "\n",
    "\n",
    "    tp.update_state(y_label, y_pred)\n",
    "    tn.update_state(y_label, y_pred)\n",
    "    fp.update_state(y_label, y_pred)\n",
    "    fn.update_state(y_label, y_pred)\n",
    "\n",
    "    tp = tp.result().numpy()\n",
    "    tn = tn.result().numpy()\n",
    "    fp = fp.result().numpy()\n",
    "    fn = fn.result().numpy()\n",
    "    loss_MCC = (tp*tn - fp*fn)/ math.sqrt(   (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)  )\n",
    "    return loss_MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "mk1krM3U8AHH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-17 09:02:55.146057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10376 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:81:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 32\n",
    "epochs = 200\n",
    "optimizer=Adam(learning_rate=1e-4)\n",
    "loss=['mean_squared_error', \"binary_crossentropy\"]\n",
    "\n",
    "hypers = [batch_size, epochs, optimizer, loss]\n",
    "# model.compile(optimizer= optimizer, loss= loss)\n",
    "\n",
    "# batch_size, epochs, optimizer, loss = hyper_choices\n",
    "model_type1 = \"SFCNN\"\n",
    "# save_path1 = './Save'\n",
    "# best_path1 = './Best'\n",
    "sfcnn_save_path = './SFCNN/Save'\n",
    "sfcnn_best_path = './SFCNN/Best'\n",
    "\n",
    "model_type2 = \"3DCNN\"\n",
    "_3dcnn_save_path = './3DCNN/Save'\n",
    "_3dcnn_best_path = './3DCNN/Best'\n",
    "\n",
    "# model_type2 = \"3DCNN\"\n",
    "test_save_path = './test/Save'\n",
    "test_best_path = './test/Best'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "eO5cJeMjHNuu"
   },
   "outputs": [],
   "source": [
    "csv_path1_100 = './SFCNN/Report/resultSFCNN100.csv'\n",
    "csv_path1_500 = './SFCNN/Report/resultSFCNN500.csv'\n",
    "csv_path1_2000 = './SFCNN/Report/resultSFCNN2000.csv'\n",
    "\n",
    "csv_path2_100 = './3DCNN/Report/result3DCNN100.csv'\n",
    "csv_path2_500 = './3DCNN/Report/result3DCNN500.csv'\n",
    "csv_path2_2000 = './3DCNN/Report/result3DCNN2000.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "S14hwNSVvPcc"
   },
   "outputs": [],
   "source": [
    "csv_path3_100 = './test/Report/result3DCNN100.csv'\n",
    "csv_path3_500 = './test/Report/result3DCNN500.csv'\n",
    "csv_path3_2000 = './test/Report/result3DCNN2000.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCbRapmUzYVQ"
   },
   "source": [
    "## See Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "iipob07vMZ7O"
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CRLgUZR2zc1P",
    "outputId": "88062a51-a751-4666-a16c-b195b93508ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) for `plot_model` to work.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:396: UserWarning: Skipping variable loading for optimizer 'adam', because it has 122 variables whereas the saved optimizer has 2 variables. \n",
      "  trackable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "test3DCNNModel = keras.models.load_model(test_save_path + '.keras')\n",
    "# test3DCNNModel.summary()\n",
    "plot_model(test3DCNNModel, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fbrSMO5K599T",
    "outputId": "00385a40-cfa9-4bb7-ed4f-fb04f91b4ef5"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File not found: filepath=./SFCNN/Save.keras. Please ensure the file is an accessible `.keras` zip file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m testSFCNNModel \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msfcnn_save_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.keras\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# testSFCNNModel.summary()\u001b[39;00m\n\u001b[1;32m      3\u001b[0m plot_model(testSFCNNModel, to_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_plot.png\u001b[39m\u001b[38;5;124m'\u001b[39m, show_shapes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, show_layer_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/keras/src/saving/saving_api.py:185\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format\u001b[38;5;241m.\u001b[39mload_model_from_hdf5(filepath)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    186\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    189\u001b[0m     )\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile format not supported: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    193\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeras 3 only supports V3 `.keras` files and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmight have a different name).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    203\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: File not found: filepath=./SFCNN/Save.keras. Please ensure the file is an accessible `.keras` zip file."
     ]
    }
   ],
   "source": [
    "testSFCNNModel = keras.models.load_model(sfcnn_save_path + '.keras')\n",
    "# testSFCNNModel.summary()\n",
    "plot_model(testSFCNNModel, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfRS5Oxaivar"
   },
   "source": [
    "## Create Comparision Reg Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xncZTWD2pjO4"
   },
   "outputs": [],
   "source": [
    "def run_plots(result_model):\n",
    "  plot_reg(result_model[0], result_model[1])\n",
    "  plot_class(result_model[2], result_model[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euPzj7K1A4Hu"
   },
   "source": [
    "## Run training SFCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rhewkwQS11eD",
    "outputId": "83f8fb5d-be69-428e-9bff-ef2f98e721d5"
   },
   "outputs": [],
   "source": [
    "resultSFCNN1 = exportVal(val_list_IDs, p_folder, l_folder, la_folder, batch_size, epochs, sfcnn_save_path, sfcnn_best_path, model_type1, csv_path1_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 863
    },
    "id": "-OHgnO_Homzu",
    "outputId": "f9f9af17-3881-4e90-8888-316c4800dabb"
   },
   "outputs": [],
   "source": [
    "# plot_reg(resultSFCNN1[0], resultSFCNN1[1])\n",
    "# plot_class(resultSFCNN1[2], resultSFCNN1[3])\n",
    "run_plots(resultSFCNN1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "APJOAr7ukKIh",
    "outputId": "422fd7fa-3384-46c8-bb6a-2448abd557a7"
   },
   "outputs": [],
   "source": [
    "resultSFCNN2 = exportVal(test_list_IDs, p_folder, l_folder, la_folder, batch_size, epochs, sfcnn_save_path, sfcnn_best_path, model_type1, csv_path1_500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 863
    },
    "id": "Y307i4Zio0iB",
    "outputId": "534cd43f-2ac0-4f54-b4e4-1cbc1ca35835"
   },
   "outputs": [],
   "source": [
    "# plot_reg(resultSFCNN2[0], resultSFCNN2[1])\n",
    "# plot_class(resultSFCNN2[2], resultSFCNN2[3])\n",
    "run_plots(resultSFCNN2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kay9BVgeG9CJ",
    "outputId": "21ecf01e-cf2d-46ac-99bc-563396623221"
   },
   "outputs": [],
   "source": [
    "resultSFCNN3 = exportVal(last_list_IDs, p_folder, l_folder, la_folder, batch_size, epochs, sfcnn_save_path, sfcnn_best_path, model_type1, csv_path1_2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 863
    },
    "id": "lTpwLFa3o9KP",
    "outputId": "258a6cc4-14a7-45b4-f6e1-9d53a26a8264"
   },
   "outputs": [],
   "source": [
    "# plot_reg(resultSFCNN3[0], resultSFCNN3[1])\n",
    "# plot_class(resultSFCNN3[2], resultSFCNN3[3])\n",
    "run_plots(resultSFCNN3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y91XD4PyEGIr"
   },
   "outputs": [],
   "source": [
    "# model_test = keras.models.load_model(save_path1)\n",
    "# model_test.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOG3-L127hvh"
   },
   "source": [
    "## Run training 3DCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dvR4mW2s7hg7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------- Start TrainDataset ------------------------\n",
      "=======================Batch 1==============================\n",
      "Get dataset\n",
      "----------------------- Train TrainDataset ------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1715936610.532751    1090 service.cc:145] XLA service 0x73b2b0002f50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1715936610.532851    1090 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3060, Compute Capability 8.6\n",
      "2024-05-17 09:03:30.879264: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-05-17 09:03:31.895956: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1715936614.151672    1324 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_2511', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "I0000 00:00:1715936614.232199    1350 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_2511', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "I0000 00:00:1715936614.349932    1318 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_5849', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "I0000 00:00:1715936614.409872    1321 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_5849', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "I0000 00:00:1715936614.519687    1327 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_5849', 40 bytes spill stores, 40 bytes spill loads\n",
      "\n",
      "2024-05-17 09:03:46.871418: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng0{} for conv (f32[32,128,27,27,27]{4,3,2,1,0}, u8[0]{0}) custom-call(f32[32,64,26,26,26]{4,3,2,1,0}, f32[64,128,2,2,2]{4,3,2,1,0}), window={size=2x2x2}, dim_labels=bf012_oi012->bf012, custom_call_target=\"__cudnn$convBackwardInput\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0}} is taking a while...\n",
      "2024-05-17 09:03:46.923875: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.052664701s\n",
      "Trying algorithm eng0{} for conv (f32[32,128,27,27,27]{4,3,2,1,0}, u8[0]{0}) custom-call(f32[32,64,26,26,26]{4,3,2,1,0}, f32[64,128,2,2,2]{4,3,2,1,0}), window={size=2x2x2}, dim_labels=bf012_oi012->bf012, custom_call_target=\"__cudnn$convBackwardInput\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0}} is taking a while...\n",
      "I0000 00:00:1715936646.493393    1090 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save\n",
      "======================= End Batch 1==============================\n",
      "=======================Batch 2==============================\n",
      "Get dataset\n",
      "----------------------- Train TrainDataset ------------------------\n",
      "Save\n",
      "======================= End Batch 2==============================\n",
      "=======================Batch 3==============================\n",
      "Get dataset\n",
      "----------------------- Train TrainDataset ------------------------\n",
      "Save\n",
      "======================= End Batch 3==============================\n",
      "=======================Batch 4==============================\n",
      "Get dataset\n",
      "----------------------- Train TrainDataset ------------------------\n",
      "Save\n",
      "======================= End Batch 4==============================\n",
      "=======================Batch 5==============================\n",
      "Get dataset\n",
      "----------------------- Train TrainDataset ------------------------\n",
      "Save\n",
      "======================= End Batch 5==============================\n",
      "=======================Batch 6==============================\n",
      "Get dataset\n",
      "----------------------- Train TrainDataset ------------------------\n",
      "Save\n",
      "======================= End Batch 6==============================\n",
      "=======================Batch 7==============================\n",
      "Get dataset\n",
      "----------------------- Train TrainDataset ------------------------\n",
      "Save\n",
      "======================= End Batch 7==============================\n",
      "=======================Batch 8==============================\n",
      "Get dataset\n",
      "----------------------- Train TrainDataset ------------------------\n"
     ]
    }
   ],
   "source": [
    "hyper_train(train_list_IDs, p_folder, l_folder, la_folder, batch_size, epochs, test_save_path, test_best_path, model_type2, hypers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T04zZrp3Cn4S"
   },
   "outputs": [],
   "source": [
    "# exportVal(val_list_IDs, p_folder, l_folder, la_folder, batch_size, epochs, save_path2, best_path2, model_type2, csv_path3_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BVCj1RzdvEtP",
    "outputId": "4d753c60-0eb9-49a9-bccf-574e0136b193"
   },
   "outputs": [],
   "source": [
    "result3DCNN4 = exportVal(val_list_IDs, p_folder, l_folder, la_folder, batch_size, epochs, test_save_path, test_best_path, model_type2, csv_path3_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "id": "rkOAm8T0pyRD",
    "outputId": "8d8cd5ea-3474-429b-f2ac-ae158c490dab"
   },
   "outputs": [],
   "source": [
    "run_plots(result3DCNN4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LsUBo9XZ1qOq",
    "outputId": "9fac815f-e27c-4b61-8d5c-46888a507a55"
   },
   "outputs": [],
   "source": [
    "result3DCNN5 = exportVal(test_list_IDs, p_folder, l_folder, la_folder, batch_size, epochs, test_save_path, test_best_path, model_type2, csv_path3_500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 868
    },
    "id": "fxaKDm5Tp4iO",
    "outputId": "2a73556c-2e6d-4803-aceb-5be3d769b1bc"
   },
   "outputs": [],
   "source": [
    "run_plots(result3DCNN5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C-EcWIRAjgpJ",
    "outputId": "701f16c3-28ca-46e0-fbbe-9da19a1145f4"
   },
   "outputs": [],
   "source": [
    "result3DCNN6 = exportVal(last_list_IDs, p_folder, l_folder, la_folder, batch_size, epochs, test_save_path, test_best_path, model_type2, csv_path3_2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 863
    },
    "id": "ZRzmEnGUp7z6",
    "outputId": "1faeb0bc-e3d9-46c5-f766-92bce0ee4a4d"
   },
   "outputs": [],
   "source": [
    "run_plots(result3DCNN6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "586RBisaqg5u",
    "outputId": "10657378-010a-4e88-bdfb-2c988112e95b"
   },
   "outputs": [],
   "source": [
    "model_test = keras.models.load_model(test_save_path)\n",
    "model_test.summary()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "yq6rr80ME2JU",
    "jP70igtsHsUf",
    "wwckd8Tluiub",
    "BTqd63HuAWY9",
    "yCbRapmUzYVQ",
    "mfRS5Oxaivar",
    "v-BA8DBDedUY",
    "t2zohTv1qp3z",
    "xZoEupd1Wgx7",
    "pf_P_RsChmJn"
   ],
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
